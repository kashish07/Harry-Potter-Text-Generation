{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text generation lstm",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3Z0vqXt2EL2"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39OQJbpBvoYA"
      },
      "source": [
        "def read_text(path):\n",
        "  with open(path) as f:\n",
        "    text = f.read()\n",
        "  return text"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-iMVZKywk_d"
      },
      "source": [
        "def sep_punc(text):\n",
        "  toks = []\n",
        "  for token in nlp(text):\n",
        "    if token.text not in '\\n\\n\\n!\"#$%&()*+,-./:;<  --=>?@[\\\\]^_`{|}~\\t\\n':\n",
        "      toks.append(token.text.lower())   \n",
        "  return toks   "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usT3Ru87xk6o"
      },
      "source": [
        "import spacy \n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTI0njyCxNmY"
      },
      "source": [
        "# path = \"/content/moby_dick_four_chapters.txt\"\n",
        "path = '/content/hp p1.txt'\n",
        "file_text = read_text(path)\n",
        "tokens = sep_punc(file_text)\n",
        "# print(tokens )"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZguvxUhxiOX"
      },
      "source": [
        "train_len = 40 + 1\n",
        "text_sequences = []\n",
        "for i in range(train_len , len(tokens)):\n",
        "  seq = tokens[i - train_len : i]\n",
        "  text_sequences.append(seq)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCS1ot_EzwmS"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(filters = '\\n\\n\\n!\"#$%&()*+,-./:;<  --=>?@[\\\\]^_`{|}~\\t\\n')\n",
        "tokenizer.fit_on_texts(text_sequences)\n",
        "sequences = tokenizer.texts_to_sequences(text_sequences)\n",
        "# print(sequences)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYOqTaLR0zju"
      },
      "source": [
        "ind_word = tokenizer.index_word"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPSe9tFzVdyf"
      },
      "source": [
        "ind_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mWyaA-K1Hni",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8474e238-fce4-46b9-d271-4a1bc1256938"
      },
      "source": [
        "vocab_size = len(tokenizer.word_counts)\n",
        "vocab_size"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5860"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baY04vrz1YCC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "060404a6-d736-4767-a4f2-f34621a77899"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "sequences = np.array(sequences)\n",
        "X = sequences[: , :-1]\n",
        "y = sequences[: , -1]\n",
        "y = to_categorical(y , num_classes = vocab_size + 1) # + 1 for holding zeroth class\n",
        "\n",
        "seq_len = X.shape[1]\n",
        "X.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(81197, 40)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6JT0knb3ymc"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense , Embedding , LSTM , Dropout, Flatten"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufV1kHPV2BXI"
      },
      "source": [
        "def create_model(vocab_size , seq_len):\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(input_dim = vocab_size , output_dim= seq_len * 2, input_length = seq_len))\n",
        "  model.add(LSTM(units= seq_len * 2 , return_sequences= True))\n",
        "  # model.add(LSTM(units = seq_len*2))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(seq_len , activation = 'relu'))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(vocab_size , activation = 'softmax'))\n",
        "  model.compile(loss = 'categorical_crossentropy' , optimizer = 'adam', metrics = ['accuracy'])\n",
        "  model.summary()\n",
        "  return model"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJqtuVlB3XVf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41c47d80-6884-4bd2-a9c8-f456575365f3"
      },
      "source": [
        "model = create_model(vocab_size + 1 , seq_len)\n",
        "model.fit(X, y , batch_size = 100 , epochs = 100)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 40, 80)            468880    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 40, 80)            51520     \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 3200)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 40)                128040    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 40)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 5861)              240301    \n",
            "=================================================================\n",
            "Total params: 888,741\n",
            "Trainable params: 888,741\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "812/812 [==============================] - 20s 15ms/step - loss: 6.9039 - accuracy: 0.0413\n",
            "Epoch 2/100\n",
            "812/812 [==============================] - 11s 14ms/step - loss: 6.3981 - accuracy: 0.0432\n",
            "Epoch 3/100\n",
            "812/812 [==============================] - 11s 14ms/step - loss: 6.2753 - accuracy: 0.0458\n",
            "Epoch 4/100\n",
            "812/812 [==============================] - 11s 14ms/step - loss: 6.0491 - accuracy: 0.0550\n",
            "Epoch 5/100\n",
            "812/812 [==============================] - 12s 14ms/step - loss: 5.7877 - accuracy: 0.0736\n",
            "Epoch 6/100\n",
            "812/812 [==============================] - 12s 15ms/step - loss: 5.5480 - accuracy: 0.0887\n",
            "Epoch 7/100\n",
            "812/812 [==============================] - 12s 15ms/step - loss: 5.3438 - accuracy: 0.0990\n",
            "Epoch 8/100\n",
            "812/812 [==============================] - 12s 15ms/step - loss: 5.1263 - accuracy: 0.1140\n",
            "Epoch 9/100\n",
            "812/812 [==============================] - 12s 14ms/step - loss: 4.9456 - accuracy: 0.1236\n",
            "Epoch 10/100\n",
            "812/812 [==============================] - 12s 15ms/step - loss: 4.7703 - accuracy: 0.1351\n",
            "Epoch 11/100\n",
            "812/812 [==============================] - 12s 15ms/step - loss: 4.6204 - accuracy: 0.1406\n",
            "Epoch 12/100\n",
            "812/812 [==============================] - 12s 15ms/step - loss: 4.4751 - accuracy: 0.1494\n",
            "Epoch 13/100\n",
            "812/812 [==============================] - 12s 15ms/step - loss: 4.3226 - accuracy: 0.1600\n",
            "Epoch 14/100\n",
            "812/812 [==============================] - 12s 15ms/step - loss: 4.2068 - accuracy: 0.1671\n",
            "Epoch 15/100\n",
            "812/812 [==============================] - 12s 15ms/step - loss: 4.0713 - accuracy: 0.1778\n",
            "Epoch 16/100\n",
            "812/812 [==============================] - 12s 15ms/step - loss: 3.9422 - accuracy: 0.1897\n",
            "Epoch 17/100\n",
            "812/812 [==============================] - 12s 15ms/step - loss: 3.8356 - accuracy: 0.1995\n",
            "Epoch 18/100\n",
            "812/812 [==============================] - 12s 15ms/step - loss: 3.7258 - accuracy: 0.2124\n",
            "Epoch 19/100\n",
            "812/812 [==============================] - 12s 14ms/step - loss: 3.6129 - accuracy: 0.2249\n",
            "Epoch 20/100\n",
            "812/812 [==============================] - 11s 14ms/step - loss: 3.5131 - accuracy: 0.2362\n",
            "Epoch 21/100\n",
            "812/812 [==============================] - 12s 14ms/step - loss: 3.4312 - accuracy: 0.2464\n",
            "Epoch 22/100\n",
            "812/812 [==============================] - 12s 14ms/step - loss: 3.3351 - accuracy: 0.2603\n",
            "Epoch 23/100\n",
            "812/812 [==============================] - 12s 15ms/step - loss: 3.2313 - accuracy: 0.2739\n",
            "Epoch 24/100\n",
            "812/812 [==============================] - 12s 15ms/step - loss: 3.1546 - accuracy: 0.2824\n",
            "Epoch 25/100\n",
            "812/812 [==============================] - 12s 14ms/step - loss: 3.0803 - accuracy: 0.2935\n",
            "Epoch 26/100\n",
            "812/812 [==============================] - 11s 14ms/step - loss: 3.0015 - accuracy: 0.3099\n",
            "Epoch 27/100\n",
            "812/812 [==============================] - 12s 14ms/step - loss: 2.9341 - accuracy: 0.3209\n",
            "Epoch 28/100\n",
            "812/812 [==============================] - 12s 14ms/step - loss: 2.8760 - accuracy: 0.3285\n",
            "Epoch 29/100\n",
            "812/812 [==============================] - 12s 14ms/step - loss: 2.8062 - accuracy: 0.3379\n",
            "Epoch 30/100\n",
            "812/812 [==============================] - 11s 14ms/step - loss: 2.7454 - accuracy: 0.3488\n",
            "Epoch 31/100\n",
            "812/812 [==============================] - 12s 15ms/step - loss: 2.6977 - accuracy: 0.3540\n",
            "Epoch 32/100\n",
            "812/812 [==============================] - 11s 14ms/step - loss: 2.6486 - accuracy: 0.3658\n",
            "Epoch 33/100\n",
            "812/812 [==============================] - 11s 14ms/step - loss: 2.5776 - accuracy: 0.3751\n",
            "Epoch 34/100\n",
            "812/812 [==============================] - 11s 14ms/step - loss: 2.5520 - accuracy: 0.3795\n",
            "Epoch 35/100\n",
            "812/812 [==============================] - 11s 14ms/step - loss: 2.4983 - accuracy: 0.3900\n",
            "Epoch 36/100\n",
            "812/812 [==============================] - 12s 14ms/step - loss: 2.4618 - accuracy: 0.3970\n",
            "Epoch 37/100\n",
            "812/812 [==============================] - 12s 14ms/step - loss: 2.4104 - accuracy: 0.4051\n",
            "Epoch 38/100\n",
            "812/812 [==============================] - 12s 14ms/step - loss: 2.3686 - accuracy: 0.4113\n",
            "Epoch 39/100\n",
            "812/812 [==============================] - 12s 15ms/step - loss: 2.3177 - accuracy: 0.4192\n",
            "Epoch 40/100\n",
            "812/812 [==============================] - 11s 14ms/step - loss: 2.2922 - accuracy: 0.4263\n",
            "Epoch 41/100\n",
            "812/812 [==============================] - 12s 14ms/step - loss: 2.2566 - accuracy: 0.4344\n",
            "Epoch 42/100\n",
            "812/812 [==============================] - 11s 14ms/step - loss: 2.2088 - accuracy: 0.4431\n",
            "Epoch 43/100\n",
            "812/812 [==============================] - 11s 14ms/step - loss: 2.1859 - accuracy: 0.4475\n",
            "Epoch 44/100\n",
            "812/812 [==============================] - 11s 14ms/step - loss: 2.1396 - accuracy: 0.4547\n",
            "Epoch 45/100\n",
            "812/812 [==============================] - 11s 14ms/step - loss: 2.1172 - accuracy: 0.4609\n",
            "Epoch 46/100\n",
            "812/812 [==============================] - 11s 14ms/step - loss: 2.0934 - accuracy: 0.4629\n",
            "Epoch 47/100\n",
            "812/812 [==============================] - 11s 14ms/step - loss: 2.0518 - accuracy: 0.4720\n",
            "Epoch 48/100\n",
            "812/812 [==============================] - 12s 15ms/step - loss: 2.0337 - accuracy: 0.4747\n",
            "Epoch 49/100\n",
            "812/812 [==============================] - 11s 14ms/step - loss: 2.0091 - accuracy: 0.4790\n",
            "Epoch 50/100\n",
            "812/812 [==============================] - 11s 14ms/step - loss: 1.9835 - accuracy: 0.4886\n",
            "Epoch 51/100\n",
            "812/812 [==============================] - 11s 14ms/step - loss: 1.9468 - accuracy: 0.4924\n",
            "Epoch 52/100\n",
            "812/812 [==============================] - 12s 15ms/step - loss: 1.9418 - accuracy: 0.4930\n",
            "Epoch 53/100\n",
            "812/812 [==============================] - 11s 14ms/step - loss: 1.9035 - accuracy: 0.5026\n",
            "Epoch 54/100\n",
            "812/812 [==============================] - 12s 15ms/step - loss: 1.8849 - accuracy: 0.5086\n",
            "Epoch 55/100\n",
            "812/812 [==============================] - 12s 15ms/step - loss: 1.8732 - accuracy: 0.5079\n",
            "Epoch 56/100\n",
            "812/812 [==============================] - 11s 14ms/step - loss: 1.8552 - accuracy: 0.5108\n",
            "Epoch 57/100\n",
            "812/812 [==============================] - 11s 14ms/step - loss: 1.8277 - accuracy: 0.5196\n",
            "Epoch 58/100\n",
            "812/812 [==============================] - 12s 15ms/step - loss: 1.8054 - accuracy: 0.5235\n",
            "Epoch 59/100\n",
            "812/812 [==============================] - 12s 15ms/step - loss: 1.7799 - accuracy: 0.5326\n",
            "Epoch 60/100\n",
            "812/812 [==============================] - 12s 15ms/step - loss: 1.7591 - accuracy: 0.5323\n",
            "Epoch 61/100\n",
            "812/812 [==============================] - 11s 14ms/step - loss: 1.7651 - accuracy: 0.5330\n",
            "Epoch 62/100\n",
            "812/812 [==============================] - 11s 14ms/step - loss: 1.7183 - accuracy: 0.5405\n",
            "Epoch 63/100\n",
            "812/812 [==============================] - 11s 14ms/step - loss: 1.7236 - accuracy: 0.5410\n",
            "Epoch 64/100\n",
            "812/812 [==============================] - 11s 14ms/step - loss: 1.6885 - accuracy: 0.5466\n",
            "Epoch 65/100\n",
            "812/812 [==============================] - 12s 14ms/step - loss: 1.6841 - accuracy: 0.5508\n",
            "Epoch 66/100\n",
            "812/812 [==============================] - 11s 14ms/step - loss: 1.6807 - accuracy: 0.5514\n",
            "Epoch 67/100\n",
            "812/812 [==============================] - 11s 14ms/step - loss: 1.6628 - accuracy: 0.5579\n",
            "Epoch 68/100\n",
            "812/812 [==============================] - 11s 14ms/step - loss: 1.6518 - accuracy: 0.5582\n",
            "Epoch 69/100\n",
            "812/812 [==============================] - 11s 14ms/step - loss: 1.6355 - accuracy: 0.5615\n",
            "Epoch 70/100\n",
            "812/812 [==============================] - 11s 13ms/step - loss: 1.6169 - accuracy: 0.5664\n",
            "Epoch 71/100\n",
            "812/812 [==============================] - 11s 14ms/step - loss: 1.6023 - accuracy: 0.5694\n",
            "Epoch 72/100\n",
            "812/812 [==============================] - 11s 14ms/step - loss: 1.6023 - accuracy: 0.5691\n",
            "Epoch 73/100\n",
            "812/812 [==============================] - 11s 14ms/step - loss: 1.5967 - accuracy: 0.5682\n",
            "Epoch 74/100\n",
            "812/812 [==============================] - 11s 14ms/step - loss: 1.5757 - accuracy: 0.5739\n",
            "Epoch 75/100\n",
            "812/812 [==============================] - 12s 14ms/step - loss: 1.5519 - accuracy: 0.5798\n",
            "Epoch 76/100\n",
            "812/812 [==============================] - 11s 14ms/step - loss: 1.5470 - accuracy: 0.5800\n",
            "Epoch 77/100\n",
            "812/812 [==============================] - 11s 14ms/step - loss: 1.5208 - accuracy: 0.5866\n",
            "Epoch 78/100\n",
            "812/812 [==============================] - 11s 14ms/step - loss: 1.5096 - accuracy: 0.5889\n",
            "Epoch 79/100\n",
            "812/812 [==============================] - 12s 14ms/step - loss: 1.5220 - accuracy: 0.5883\n",
            "Epoch 80/100\n",
            "812/812 [==============================] - 12s 14ms/step - loss: 1.5169 - accuracy: 0.5869\n",
            "Epoch 81/100\n",
            "812/812 [==============================] - 11s 14ms/step - loss: 1.4876 - accuracy: 0.5952\n",
            "Epoch 82/100\n",
            "812/812 [==============================] - 11s 14ms/step - loss: 1.4918 - accuracy: 0.5944\n",
            "Epoch 83/100\n",
            "812/812 [==============================] - 11s 14ms/step - loss: 1.4909 - accuracy: 0.5947\n",
            "Epoch 84/100\n",
            "812/812 [==============================] - 12s 14ms/step - loss: 1.4723 - accuracy: 0.5994\n",
            "Epoch 85/100\n",
            "812/812 [==============================] - 11s 14ms/step - loss: 1.4735 - accuracy: 0.5982\n",
            "Epoch 86/100\n",
            "812/812 [==============================] - 12s 14ms/step - loss: 1.4700 - accuracy: 0.6032\n",
            "Epoch 87/100\n",
            "812/812 [==============================] - 12s 15ms/step - loss: 1.4506 - accuracy: 0.6040\n",
            "Epoch 88/100\n",
            "812/812 [==============================] - 12s 15ms/step - loss: 1.4418 - accuracy: 0.6039\n",
            "Epoch 89/100\n",
            "812/812 [==============================] - 12s 15ms/step - loss: 1.4354 - accuracy: 0.6066\n",
            "Epoch 90/100\n",
            "812/812 [==============================] - 12s 15ms/step - loss: 1.4117 - accuracy: 0.6149\n",
            "Epoch 91/100\n",
            "812/812 [==============================] - 12s 14ms/step - loss: 1.4192 - accuracy: 0.6102\n",
            "Epoch 92/100\n",
            "812/812 [==============================] - 12s 15ms/step - loss: 1.3955 - accuracy: 0.6145\n",
            "Epoch 93/100\n",
            "812/812 [==============================] - 12s 15ms/step - loss: 1.3992 - accuracy: 0.6176\n",
            "Epoch 94/100\n",
            "812/812 [==============================] - 11s 14ms/step - loss: 1.4015 - accuracy: 0.6140\n",
            "Epoch 95/100\n",
            "812/812 [==============================] - 12s 15ms/step - loss: 1.4016 - accuracy: 0.6185\n",
            "Epoch 96/100\n",
            "812/812 [==============================] - 12s 15ms/step - loss: 1.3779 - accuracy: 0.6191\n",
            "Epoch 97/100\n",
            "812/812 [==============================] - 12s 14ms/step - loss: 1.3882 - accuracy: 0.6204\n",
            "Epoch 98/100\n",
            "812/812 [==============================] - 11s 14ms/step - loss: 1.3622 - accuracy: 0.6235\n",
            "Epoch 99/100\n",
            "812/812 [==============================] - 12s 14ms/step - loss: 1.3726 - accuracy: 0.6223\n",
            "Epoch 100/100\n",
            "812/812 [==============================] - 12s 15ms/step - loss: 1.3616 - accuracy: 0.6262\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3dc7e11e50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Lo7z7gYi6r0"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "def generate_text(model , tokenizer , seq_len , seed_text, no_of_words):\n",
        "  output_text = []\n",
        "  input_text = seed_text\n",
        "\n",
        "  for i in range(no_of_words):\n",
        "    encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
        "    pad_encoded = pad_sequences([encoded_text] , maxlen = seq_len , truncating = 'pre')\n",
        "\n",
        "    ind = np.argmax(model.predict(pad_encoded) , axis = 1)[0]\n",
        "    # type(ind)\n",
        "    word = tokenizer.index_word[ind]\n",
        "    input_text += ' ' + word\n",
        "\n",
        "    output_text.append(word)\n",
        "\n",
        "  return ' '.join(output_text)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ufmp-yjBkh8X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "60b20505-4208-4dd5-87e9-cc8e331872f8"
      },
      "source": [
        "import random\n",
        "r_p = random.randint(0 , len(text_sequences))\n",
        "rand_text = text_sequences[r_p]\n",
        "seed_text = ' '.join(rand_text)\n",
        "\n",
        "# seed_text\n",
        "new = generate_text(model , tokenizer ,seq_len , seed_text , 30)\n",
        "# print(seed_text + \"\\n\" + new )\n",
        "seed_text"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'obviously in one of the towers they found their beds at last five four posters hung with deep red velvet curtains their trunks had already been brought up too tired to talk much they pulled on their pajamas and fell into'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LB0BEq5hzkHu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bd9bdd02-bd0c-4c4e-a01b-d2ec0c3013f0"
      },
      "source": [
        "# seed_text = 'harry did not wanted to join the slytherine he knew about people who were particularly bad and belonged to slytherine'\n",
        "generate_text(model , tokenizer , seq_len , seed_text , 20)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"bed great food is n't it ron muttered to harry through the shadow and his face harry was moving lee\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EfcXySAN5zz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}